# -*- coding: utf-8 -*-
"""lr_difference.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zKQvLaaeTsRwisoihfGlADNB9hQL65oq
"""

import numpy as np


X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 5, 4, 5])

# Ordinary Least Squares (OLS) method
def linear_regression_ols(X, y):
    X_ols = np.column_stack((np.ones_like(X), X))
    beta_ols = np.linalg.inv(X_ols.T @ X_ols) @ X_ols.T @ y
    return X_ols @ beta_ols

residuals_ols = y - linear_regression_ols(X, y)

# Numerical optimization with gradient descent
def linear_regression_gradient_descent(X, y, learning_rate=0.01, num_iterations=1000):
    X_gd = np.column_stack((np.ones_like(X), X))
    beta_gd = np.zeros(X_gd.shape[1])

    for _ in range(num_iterations):
        y_pred_gd = X_gd @ beta_gd
        gradient_gd = -2 * X_gd.T @ (y - y_pred_gd)
        beta_gd -= learning_rate * gradient_gd

    return X_gd @ beta_gd

residuals_gradient_descent = y - linear_regression_gradient_descent(X, y)

# Calculate the absolute difference between the residuals
absolute_difference = np.abs(residuals_ols - residuals_gradient_descent)

# Print the results
print("Residuals using OLS method:", residuals_ols)
print("Residuals using Gradient Descent method:", residuals_gradient_descent)
print("Absolute difference between residuals:", absolute_difference)