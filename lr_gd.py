# -*- coding: utf-8 -*-
"""lr_gd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BlkYYZX6CmH4mQJg_u8zwzwPrkNA2ieY
"""

import numpy as np

def linear_regression_gradient_descent(X, y, learning_rate=0.01, num_iterations=1000):
    # Add a column of ones to the feature matrix for the intercept term
    X = np.column_stack((np.ones_like(X), X))

    # Initialize coefficients with zeros
    beta = np.zeros(X.shape[1])

    # Perform gradient descent
    for _ in range(num_iterations):
        # Calculate predicted values
        y_pred = X @ beta

        # Calculate the gradient of the cost function
        gradient = -2 * X.T @ (y - y_pred)

        # Update coefficients using gradient descent
        beta -= learning_rate * gradient

    return beta

def get_residuals_gradient_descent(X, y, beta):
    # Add a column of ones to the feature matrix for the intercept term
    X = np.column_stack((np.ones_like(X), X))

    # Calculate the predicted values using the obtained coefficients
    y_pred = X @ beta

    # Calculate the residuals (epsilon) as the difference between observed and predicted values
    residuals = y - y_pred

    return residuals

X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 5, 4, 5])

# Performing linear regression with gradient descent
coefficients_gd = linear_regression_gradient_descent(X, y)

# Getting residuals
residuals_gd = get_residuals_gradient_descent(X, y, coefficients_gd)

# Print the coefficients and residuals
print("Coefficients (beta) using gradient descent:", coefficients_gd)
print("Residuals (epsilon) using gradient descent:", residuals_gd)